<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>D3 FS Operations Heatmap Spike</title>
</head>

<style>
  body {
    font:    10px sans-serif;
  }

  .label {
    font-weight: bold;
  }

  .tile {
    shape-rendering: crispEdges;
  }

  .axis path,
  .axis line {
    fill:            none;
    stroke:          #000;
    shape-rendering: crispEdges;
  }
</style>
<body>

<script src="../../bower_components/d3/d3.min.js"></script>
<script src="../../bower_components/lodash/lodash.min.js"></script>
<script src="../../bower_components/moment/moment.js"></script>
<script src="../../bower_components/moment-timezone/builds/moment-timezone-with-data.min.js"></script>


<script>

var margin = {
  top:    20,
  right:  90,
  bottom: 90,
  left:   70
};

var hostTZ = "US/Eastern";
var width  = 1024 - margin.left - margin.right;
var height = 900  - margin.top  - margin.bottom;

var xScale = d3.time.scale().range([0, width]);

var yScale = d3.scale.linear().range([height,0]);

var colorScale = d3.scale.linear().range(["white", "steelblue"]);

var svg = d3.select("body").append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
    .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

var xAxis = d3.svg.axis()
    .scale(xScale)
    .orient("bottom")
    .ticks(20)
    .tickFormat(function(d) {
      return moment(d).tz(hostTZ).format("MM-DD-YYYY HH:mm:ss");
    });

d3.json("../../data/iolat_heatmap_data.json", function(error, data) {
  if (error) return console.warn(error);

  // Data is in form:
  // [
  //   { timestamp: <seconds in UTC>,
  //     interval_data: [
  //       {
  //         "getpage:tmpfs":[[[10000,10999],1],[[12000,12999],1]],
  //       }
  //     ]
  //   },
  //   ...
  // ]
  //
  // Note that the DTrace that collects this data does so with the following llquantize() parameters:
  // llquantize(foo, 10, 3, 11, 100)
  // BASE/FACTOR: 10
  // LOW EXPONENT: 3
  // HIGH EXPONENT: 11
  // STEP:          100
  // Which will quantize from 10^3 through 10^12 - 1, with each range having 100 steps
  // So that's a possible range of 9 orders of magnitude, with 100 steps each, so our heatmap
  // could be, at most, 900 rectangles tall per time interval.
  //
  var intervals = [];
  data.forEach(function(interval) {
    var itimestamp = interval.timestamp;
    var interval_data = interval.interval_data;
    var current_interval = {};
    interval_data.forEach(function(fsop_fstype_data) {
      Object.keys(fsop_fstype_data).forEach(function(key) {
        // key is the fsop:fstype
        // console.log(key);
        // The value is an array of latency ranges, each composed of a start/end range, and a count
        // We will eventually determine which fsop/fstype we care about, and only aggregate those
        // Data Extent:
        // minimum latency among all timestamps being considered
        // maximum latency among all timestamps being considered
        // Based on the llquantize() parameters above, this will give us an extent of (even if the
        // data is sparse, which it always should be) max exponent - min exponent * 100
        // To begin, we'll just use 9 * 100 as a constant
        fsop_fstype_data[key].forEach(function(lat_array) {
          // new_key: low_bound part of [ low_bound, upper_bound ]
          //
          var new_key = lat_array[0][0];
          // new_val: count
          var new_val = lat_array[1];
          if (new_key in current_interval) {
            current_interval[new_key][1] += new_val;
          } else {
            current_interval[new_key] = [ lat_array[0], new_val ];
          }
        });
      });
    });
    intervals.push({ timestamp: itimestamp, data: current_interval });
  });
  console.log(intervals);
  console.log(_.values(intervals[0].data));

  var samples = [];
  intervals.forEach(function(item) {
    samples.push(_.values(item.data));
  });
  console.log("SAMPLES:", samples);

  var tilesPerSample = 200

  var bucketized = bucketize( samples, {nbuckets: tilesPerSample});
  console.log(bucketized);

  normalize(bucketized, {rank: true});
  console.log(bucketized);

  var normalized_samples = [ ];

  intervals.forEach(
      function(object,index) {
        var tmp_object = {
          timestamp: object.timestamp,
          sample:    bucketized[index]
        };
        // console.log(tmp_object);
        normalized_samples.push(tmp_object);
      }
  );

  console.log(normalized_samples);

  xScale.domain(d3.extent(data.map(function(d) {return new Date(d.timestamp * 1000); })));
  yScale.domain([0,normalized_samples[0].sample.length]);
  colorScale.domain([0,1]);

  var tileHeight = height / tilesPerSample;
  var tileWidth  = width  / 31;

  var timestamps = svg.selectAll(".timestamp")
      .data(normalized_samples)
      .enter().append("g")
      .attr("class","timestamp");

  timestamps.selectAll(".tile")
      .data(function(d,i) { return normalized_samples[i].sample; })
      .enter()
      .append("rect")
      .attr("class", "tile")
      .attr("x", function(d,i,j) { return (j * tileWidth); })
      .attr("y", function(d,i) { return height - (i * tileHeight) - tileHeight; })
      .attr("height", function() { return tileHeight; })
      .attr("width",  function() { return tileWidth; })
      .attr("fill", function(d) { return colorScale(d); });

  // Add an x-axis with labels created by xScale.domain() above...
  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(0," + height + ")")
      .call(xAxis)
      .selectAll("text")
      .style("text-anchor", "end")
      .attr("dx", "-.8em")
      .attr("dy", ".15em")
      .attr("transform", function(d) {
        return "rotate(-55)"
      });

  // Add a y-axis with labels
  svg.append("g")
      .attr("class", "y axis")
      .call(d3.svg.axis().scale(yScale).orient("left"))
      .append("text")
      .attr("class", "label")
      .attr("y", 6)
      .attr("dy", ".71em")
      .attr("text-anchor", "end")
      .attr("transform", "rotate(-90)")
      .text("Value");

});



  function bucketize(dataIntervals, conf) {
    // REQUIRED
    var nbuckets = conf.nbuckets;

    // OPTIONAL
    var min = conf.hasOwnProperty('min') ? conf.min : 0;
    var max = conf.hasOwnProperty('mzx') ? conf.max : 0;

    var i, j, k;

    var low, high;
    var lowfilled, highfilled;

    // Assume data passed in for interval is an Array, rather than an Object
    if (max === 0 ) {
      //
      // If the max has not been specified, iterate over the data to determine the maximum value, then use
      // that to determined the preferred maximum for bucketization.

      for (i = 0; i < dataIntervals.length; i++) {
        for (j = 0; j < dataIntervals[i].length; j++) {
          if (dataIntervals[i][j][0][1] > max) {
            max = dataIntervals[i][j][0][1] + 1;  // One greater than
          }
        }
      }

      // At this point, the max of all data has been determined.
      // To prevent small changes in the data from causing large swings
      // in the maximum, we compute an autoscale for the max, rather than
      // just using the max as the max bucket.
      // We then reflect this in the conf Object.
      conf.max = max = autoscale(max);
    }


    var size = (max - min) / nbuckets;
    var rval = [];

    for (i = 0; i < dataIntervals.length; i++) {
      var buckets = new Array(nbuckets);
      var datum = dataIntervals[i];

      for (j = 0; j < buckets.length; j++)
        buckets[j] = 0;

      for (j = 0; j < datum.length; j++) {
        var range = datum[j][0];
        var val   = datum[j][1];
        var u;

        if (range[0] >= max || range[1] < min)
            continue;

        if (conf.weighbyrange)
            val *= range[0] + ((range[1] - range[0]) / 2);

        // Begin by normalizing our range to our buckets, expressing this in
        // terms of multiple of buckets.
        low   = (range[0] - min) / size;
        high  = ((range[1] + 1) - min) / size;

        lowfilled  = Math.floor(low) + 1;
        highfilled = Math.floor(high);

        if (highfilled < lowfilled) {
          // We can't even fill a single bucket.  The entire value assignment
          // goes to a single bucket - the one corresponding to both the high
          // and low bucket.
          buckets[highfilled] += val;
          continue;
        }

        // Determine the amount of value that corresponds to one filled bucket
        // If we do not fill an entire bucket, this may actually exceed our value
        u = (1 / (high - low)) * val;

        // Clamp the low and high to the bucket range
        if (low < 0)
            low = 0;

        if (high >= nbuckets)
            high = nbuckets - 1;

        if (highfilled > nbuckets)
            highfilled = nbuckets;

        // If low is lower than our lowest filled bucket, add in the proper
        // portion of our value to the partially filled bucket.
        if (low < lowfilled && lowfilled > 0)
            buckets[lowfilled - 1] += (lowfilled - low) * u;

        // Iterate over entirely filled buckets - if any, and add in the
        // proportion of our value that corresponds to a single bucket.
        for (k = lowfilled; k < highfilled; k++)
          buckets[k] += u;

        if (high > highfilled)
            buckets[highfilled] += (high = highfilled) * u;
      }
      rval.push(buckets);
    }
    return rval;
  }

  function autoscale(max) {
    // Start with the power of 10 that is greater than the given maximum.
    //
    // Then divide down (alterately by 5 and 2) until a value is reached such
    // that the lowest multiple of that value greater than the max is no more
    // than 25% greater.

    // This assures that even if the max fluctuates the autoscaling remains
    // stable; but it remains close enough to the original max that not many
    // buckets will be empty at the top of the range.
    //
    var pow = Math.floor(Math.log(max) / Math.log(10)) + 1,
        i;
    var round = 1;
    var ceiling = 1.25 * max;
    var divisors = [ ];

    for (i = 0; i < pow; i++) {
      divisors.push(5);
      divisors.push(2);
      round *= 10;
    }

    for (i = 0; i < divisors.length; i++) {
      var scaled = (Math.floor(max / round) + 1) * round;

      if (scaled < ceiling)
          return(scaled);

      round /= divisors[i];
    }

    return(max);
  }

  // normalize takes a map or an array of maps produced by bucketize(), and modifies
  // the data such that the values range from 0 to 1.  The normalization method is specified
  // by conf properties:
  //
  //   rank          =>  BOOLEAN: Default == true
  //                     Normalization will be based on ranking values among
  //                     all values in the map; values will first be sorted and
  //                     then assigned the value of their rank divided by the
  //                     number of values in the map.
  //
  //    linear       => BOOLEAN: Default == false
  //                    Normalization will be linear with respect to value; values
  //                    will be normalized by dividing each by the max value in the
  //                    map.
  //
  function normalize(maps, conf) {
    var values = [];
    var mapping = {};
    var i, j, m, max = 1;
    var data;

    // If we don't override these, they are the defaults
    var preprocess = function () {};
    var process    = function () {};
    var normalized = function (value) { return (value); };

    if (!conf || (!conf.rank && !conf.linear))
        conf = { rank: true };

    if (!(maps[0][0] instanceof Array))
      maps = [ maps ];

    // Handle rank normalization
    if (conf.rank) {
      preprocess = function (value) {
        // For rank style normalization, we only consider non-zero values, so
        // that zero values will remain zero.
        if (value !== 0)
          values.push(value);
      };

      process    = function () {
        values.sort(function(lhs, rhs) {
          if (lhs < rhs)
              return(1);

          if (lhs > rhs)
              return(-1);

          return (0);
        });

        for (i = 0; i < values.length; i++) {
          mapping[values[i]] =
              (values.length - i ) / values.length;

          while ((i < values.length) &&
                 (values[i + 1] == values[i]))
            i++;
        }
      };

      normalized = function(value) {
        if (value)
            return(mapping[value]);

        return(0);
      };
    }

    // Handle linear normalization
    if (conf.linear) {
      preprocess = function (value) {
        if (value > max)
            max = value;
      };

      normalized = function(value) {
        return (value / max);
      };
    }

    //
    // First, take a preprocessing pass over all data, across all maps
    //
    for (m = 0; m < maps.length; m++) {
      data = maps[m];

      for (i = 0; i < data.length; i++) {
        for (j = 0; j < data[i].length; j++)
          preprocess(data[i][j]);
      }

      process();

      for (m = 0; m < maps.length; m++) {
        data = maps[m];

        for (i = 0; i < data.length; i++) {
          for (j = 0; j < data[i].length; j++)
            data[i][j] = normalized(data[i][j]);
        }
      }
    }

  }
</script>

</body>
</html>